# -*- coding: utf-8 -*-
"""Streamlit MOM PDF.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yXYuek64R4L53GCGnto7kRmhXJO8kYVZ

**Streamlit**
"""

#!pip install pdfplumber
#!pip install streamlit
import os
import requests
import pdfplumber
import pandas as pd
import streamlit as st
import re
from datetime import datetime
import zipfile
import pytz

#from menu import menu

import os
import streamlit.components.v1 as components

st.set_page_config(page_title="MOM Company Info Scraper", page_icon="üë∑‚Äç‚ôÇÔ∏è")


def main():
    st.title("MOM Company Info Scraper")
    st.markdown(
        """This demo scrapes the MOM website for background checks on Companies of interest."""
    )

    # Function to display the current date and time
    def display_datetime():
        # Get the Singapore Time Zone
        sgt = pytz.timezone("Asia/Singapore")
        current_datetime = datetime.now(sgt).strftime("%Y-%m-%d %H:%M:%S")
        st.write("Current Date and Time:", current_datetime)

    # Function to download a PDF file from a given URL
    def download_pdf(url, filename):
        response = requests.get(url)

        # directory = os.path.dirname(filename)
        # if not os.path.exists(directory):
        #     os.makedirs(directory)

        with open(filename, "wb") as pdf_file:
            pdf_file.write(response.content)
        # st.success(f"Downloaded {filename} successfully!")

    # PDF links
    pdf_links = [
        (
            "https://www.mom.gov.sg/orca/list-of-companies-with-demerits",
            "list-of-companies-with-demerits.pdf",
        ),
        (
            "https://www.mom.gov.sg/-/media/mom/documents/safety-health/reports-stats/stop-work-orders.pdf",
            "stop-work-orders.pdf",
        ),
        (
            "https://www.mom.gov.sg/-/media/mom/documents/safety-health/reports-stats/list-of-companies-under-bus.pdf",
            "list-of-companies-under-bus.pdf",
        ),
    ]

    # # Download and store PDFs
    # for link, filename in pdf_links:
    #     download_pdf(link, "downloads/" + filename)
    
    # Get the Singapore Time Zone
    sgt = pytz.timezone("Asia/Singapore")
    current_datetime = datetime.now(sgt).strftime("%Y-%m-%d %H:%M:%S")
#    if st.button("Download All PDFs"):
    # Download and store PDFs
    for link, filename in pdf_links:
        download_pdf(link, filename)

    # Create a zip file containing all PDFs
    #current_datetime = current_datetime.replace('/', ':')
    zip_filename = current_datetime + "_all_pdfs" + ".zip"
    try:
        #with zipfile.ZipFile("downloads/" + zip_filename, "w") as zipf:
        with zipfile.ZipFile(zip_filename, "w") as zipf:
            for _, filename in pdf_links:
                zipf.write(
                    #"downloads/" + filename, arcname=current_datetime + "_" + filename
                    filename, arcname=current_datetime + "_" + filename
                )
    except Exception as e:
        st.error(f"Error occurred while creating zip file: {e}")

    # Provide a button to download the zip file
    try:
        # with open("downloads/" + zip_filename, "rb") as zip_file:
        with open(zip_filename, "rb") as zip_file:     
            zip_data = zip_file.read()

        st.download_button(
            label="Download All PDFs",
            data=zip_data,
            file_name=zip_filename,
            mime="application/zip",
        )
    except FileNotFoundError:
        st.error("Error: Zip file not found.")

    # URL input component
    # pdf_url = st.text_input("Enter PDF URL")
    pdf_url = "https://www.mom.gov.sg/orca/list-of-companies-with-demerits"

    # if st.button("Fetch and Process PDF"):
    def fetch_and_process_pdf(pdf_url):
        if pdf_url:
            try:
                # Make a GET request to fetch the PDF file
                response = requests.get(pdf_url)
                if response.status_code == 200:
                    # Process the PDF file
                    # Save the downloaded PDF file
                    with open("downloaded_file.pdf", "wb") as f:
                        f.write(response.content)

                    # Variable to store all pages' text
                    all_text = ""

                    # Open the downloaded PDF file
                    with pdfplumber.open("downloaded_file.pdf") as pdf:
                        # Extract text from each page
                        for page in pdf.pages:
                            text = page.extract_text()
                            # Append the text of each page to the variable
                            all_text += text + "\n"  # Add a newline for separation

                    # Find the index of the first instance of "\ncompany"
                    index_company = all_text.find("\ncompany")

                    # Extract the text after the first instance of "\ncompany"
                    if index_company != -1:
                        text_after_company = all_text[
                            index_company + len("\ncompany") :
                        ].strip()
                        lines_after_company = text_after_company.split("\n")
                        print(lines_after_company)

                    # Regular expression pattern to match "Updated on <Date>"
                    pattern = r"Updated on \d{2} [A-Za-z]{3} \d{4}"

                    # Initialize variables to store the updated string and the string containing "Updated on <Date>"
                    updated_data = []
                    updated_on_string = None

                    # Loop through each string in the data
                    for item in lines_after_company:
                        # Search for the pattern in the current string
                        match = re.search(pattern, item)

                        # If the pattern is found, save it to updated_on_string and remove it from the original string
                        if match:
                            updated_on_string = match.group(0)
                            item = re.sub(pattern, "", item)

                        # Append the updated string to the list
                        updated_data.append(item)

                    def split_into_columns(list_str):
                        split_list = []
                        # Flatten the nested list using list comprehension
                        # flattened_list = [string for sublist in nested_list for string in sublist]
                        for string in list_str:
                            split_string = string.split(" ")  # Split by space
                            # Check if the split string contains at least 5 elements
                            if len(split_string) >= 5:
                                first_column = split_string[1]
                                second_column = " ".join(
                                    split_string[2:-3]
                                )  # Join the second to second-last elements
                                second_last_column = split_string[
                                    -2
                                ]  # Second last column is the second-last element
                                last_column = split_string[
                                    -1
                                ]  # Last column is the last element
                                split_list.append(
                                    [
                                        first_column,
                                        second_column,
                                        second_last_column,
                                        last_column,
                                    ]
                                )
                            else:
                                # Handle the case where the string doesn't contain enough elements
                                split_list.append([string])
                                # print(f"Skipping string: {string}. Not enough elements to split into 5 columns.")
                        return split_list

                    # Create a DataFrame
                    global df
                    df = pd.DataFrame(
                        split_into_columns(updated_data),
                        columns=[
                            "UEN",
                            "Name of company",
                            "Demerit points accumulated by company",
                            "Debarment phase and period",
                        ],
                    )

                    df.dropna(how="any", axis=0, inplace=True)
                    df.reset_index(drop=True, inplace=True)

                    # Display the output
                    st.write(updated_on_string)
                    return df
                    st.write(df)

                else:
                    st.error("Failed to fetch PDF from the provided URL")
            except Exception as e:
                st.error(f"An error occurred: {e}")
        else:
            st.warning("Please enter a valid PDF URL")

    # Streamlit app
    st.title("Webscraper (MOM)")

    #st.header("Check which companies clear your requirements")

    # Create a text input box for the user to enter strings
    st.text("Enter your list of UENs (separated by commas) below:")
    uenname_input_text = st.text_area("Input", height=5)

    # st.title("Checkbox Selection App")

    #st.header("Tick the respective filters")
    # Define a list of items
    items = [
        "Number of Fatal Cases = 0",
        "Work Injury Compentation Permanent Injury = 0",
        "NOT Under SWO",
        "NOT Under BUS"
    ]

    # # Create checkboxes for each item
    # st.subheader("Select Items:")
    # selected_items = []
    # for item in items:
    #     selected = st.checkbox(item)
    #     if selected:
    #         selected_items.append(item)

    # # Display selected items
    # st.subheader("Selected Items:")
    # if selected_items:
    #     for selected_item in selected_items:
    #         st.write(selected_item)
    # else:
    #     st.write("No items selected.")

    st.subheader("Criteria used for filtering:")
    for selected_item in items:
        st.write(selected_item)

    def fetch_data(api_url):
        try:
            response = requests.get(api_url)
            response.raise_for_status()  # Raise an error for non-200 status codes
            return response.json()
        except Exception as e:
            print(f"Failed to fetch data from {api_url}: {str(e)}")
            return None

    def filter_dataframe(df):#, items):
        try:
            filtered_df = df.copy()  # Create a copy of the original DataFrame
            #selected_conditions = []  # List to store selected filter conditions

            # # Mapping each checkbox to its corresponding filter condition
            # df["Is under SWO"] = df["Is under SWO"].map({"yes": True, "no": False})
            # df["Is under BUS"] = df["Is under BUS"].map({"yes": True, "no": False})

            # # Fill any NaN values with False
            # df["Is under SWO"] = df["Is under SWO"].fillna(False)
            # df["Is under BUS"] = df["Is under BUS"].fillna(False)

            filter_actions = {
                "Number of Fatal Cases = 0": lambda filtered_df: filtered_df["Number of Fatal Cases"] == 0,
                "Work Injury Compentation Permanent Injury = 0": lambda filtered_df: filtered_df["Number of PI Cases"] == 0,
                "NOT Under SWO": lambda filtered_df: filtered_df["Is under SWO"] == "no",
                "NOT Under BUS": lambda filtered_df: filtered_df["Is under BUS"] == "no"
            }

            # # Loop through each item (checkbox)
            # for item in items:
            #     # If the checkbox is selected, add its corresponding filter condition to the list
            #     if item in filter_actions:
            #         selected_conditions.append(filter_actions[item])

            # # Apply all selected filter conditions simultaneously
            # if selected_conditions:
                # combined_condition = selected_conditions[0](
                #     df
                # )  # Call the function to get the boolean mask
                # for condition in selected_conditions[1:]:
                #     combined_condition &= condition(
                #         df
                #     )  # Call the function to get the boolean mask

            #filtered_df = filtered_df[combined_condition]
            #filtered_df = filtered_df[filter_actions]
            
            for description, filter_function in filter_actions.items():
                filtered_df = filtered_df[filter_function(filtered_df)]

            return filtered_df

        except Exception as e:
            return pd.DataFrame()

    if uenname_input_text != "":
        # Split the input text by newline to get a list of strings
        uen_name_list = [x.strip() for x in uenname_input_text.split(",")]
        #uen_name_list = uenname_input_text.strip().split(",")
        #st.write(uen_name_list)
        uen_name_list = [
            uen_name for uen_name in uen_name_list if len(uen_name) in (9, 10)
        ]
        #st.write(uen_name_list)
        # Base URL
        base_url = "https://www.mom.gov.sg/orca/api/v2/GetIndividualCompany?q={UENNAME}%2C&_=1715326178538"

        # Generate list of URLs
        url_list = [base_url.format(UENNAME=uenname) for uenname in uen_name_list]
        # st.write(url_list)

        # Initialize an empty list to store data
        data_list = []

        for api_link in url_list:
            data = fetch_data(api_link)
            if len(data["data"]) != 0:
                data_list.append(data)

        add_info = []

        for data_dict in data_list:
            if "data" in data_dict and data_dict["data"]:
                data = data_dict["data"]
                add_info.append(data)

        output = []

        # Construct a list of dictionaries

        info_list = [
            {
                "UEN": add_info[i][0]["uen"],
                "Company Name": add_info[i][0]["companyname"],
                "Number of Fatal Cases": add_info[i][0]["nooffatalcases"],
                "Number of PI Cases": add_info[i][0]["noofpicases"],
                "Bizsafe Awards": add_info[i][0]["bizsafeawards"],
                "WSH Awards": add_info[i][0]["wshawards"],
                "Is under SWO": add_info[i][0]["isunderswo"],
                "Bizsafe": add_info[i][0]["bizsafe"],
                "Is under BUS": add_info[i][0]["isunderbusprog"],
                "BUS Entry Date": add_info[i][0]["busentrydate"],
                "Updated On": add_info[i][0]["updatedon"],
            }
            for i in range(len(add_info))
        ]
        # Convert the list of dictionaries to a DataFrame
        df_info_list = pd.DataFrame(info_list)

        # Display the current date and time
        display_datetime()

        # Display the list of strings
        st.subheader("List of companies that meets criteria:")

        filtered_df = filter_dataframe(df_info_list)#, items)

        df = fetch_and_process_pdf(pdf_url)    

        # Merge df's demerit column into filtered_df based on UEN
        filtered_df = pd.merge(filtered_df, 
                               df, 
                               on='UEN', how='left')
        st.write(filtered_df)

        st.subheader("List of companies that did not meet criteria:")
        df_dropped = df_info_list.drop(filtered_df.index)

        if df_dropped.empty:
            st.write(df_dropped)
        else: 
            df_dropped = pd.merge(df_dropped, 
                               df, 
                               on='UEN', how='left')
            st.write(df_dropped)

    else:
        st.write(
            "**<span style='color:red'>Plese enter the relevant UENs**</span>",
            unsafe_allow_html=True,
        )

    # Display the filter options
    st.title("PDF Scraper \n")

    df = fetch_and_process_pdf(pdf_url)

    if st.button("Fetch and Process PDF"):
        df = fetch_and_process_pdf(pdf_url)

    st.header("Filter Options")

    # Determine the data type of 'Demerit points' column
    demerit_points_min = int(df["Demerit points accumulated by company"].min())
    demerit_points_max = int(df["Demerit points accumulated by company"].max())

    # Add a slider for filtering demerit points
    demerit_points_range = st.slider(
        "Demerit Points Range",
        min_value=demerit_points_min,
        max_value=demerit_points_max,
        value=(demerit_points_min, demerit_points_max),
    )

    # Filter the DataFrame based on the selected range of demerit points
    filtered_df = df[
        (
            pd.to_numeric(df["Demerit points accumulated by company"])
            >= demerit_points_range[0]
        )
        & (
            pd.to_numeric(df["Demerit points accumulated by company"])
            <= demerit_points_range[1]
        )
    ]

    # Display the filtered DataFrame
    st.write("Filtered Data:")
    st.write(filtered_df)

    # Streamlit app
    st.header("Search Names")

    input_names = st.text_input(
        "Enter names separated by commas (e.g., Singapore Construction, Low Building):"
    )

    # Function to search for names in the DataFrame
    def search_names(names):
        result = pd.DataFrame()
        for name in names.split(","):
            name = name.strip()  # Remove leading and trailing whitespaces
            pattern_regex = (
                rf"\b.*{name}.*\b"  # Regular expression pattern for partial match
            )
            matching_rows = df[
                df["Name of company"].str.contains(
                    pattern_regex, case=False, regex=True
                )
            ]
            if not matching_rows.empty:
                result = pd.concat([result, matching_rows])
        return result.drop_duplicates().reset_index(drop=True)

    # Perform search when the user submits the input
    if st.button("Search"):
        if input_names:
            result_df = search_names(input_names)
            if not result_df.empty:
                st.write("Search Result:")
                st.write(result_df)
            else:
                st.write("No matching names found.")
        else:
            st.write("Please enter names in the input field.")


if __name__ == "__main__":
    #menu()
    main()
